{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title ## 1. Setup and Imports\n",
        "\n",
        "# Install necessary libraries (uncomment if running locally/outside Colab)\n",
        "!pip install -q flax optax tiktoken einops\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "import flax.linen as nn            # Flax neural network library\n",
        "from flax.training import train_state # Helper for managing model state\n",
        "import optax                       # Optimization library\n",
        "import tiktoken                    # Tokenizer for text processing\n",
        "import functools                   # For function manipulation (e.g., partial)\n",
        "from einops import rearrange       # For tensor manipulation\n",
        "import urllib.request              # To download data\n",
        "import os                          # Operating system interactions\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Check for available hardware accelerator (GPU/TPU) and set the default device\n",
        "try:\n",
        "    # Check for TPU (primarily for Google Colab)\n",
        "    import jax.tools.colab_tpu\n",
        "    # To use TPU: Uncomment the next line and select TPU in Colab Runtime settings\n",
        "    # jax.tools.colab_tpu.setup_tpu()\n",
        "    # device = jax.devices('tpu')[0]\n",
        "    # print(\"Using TPU\")\n",
        "\n",
        "    # Check for GPU\n",
        "    # To use GPU: In Colab, go to Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
        "    if jax.devices('gpu'):\n",
        "      device = jax.devices('gpu')[0]\n",
        "      print(\"Using GPU\")\n",
        "    else:\n",
        "      # Fallback to CPU if no GPU found (and TPU not explicitly enabled)\n",
        "      device = jax.devices('cpu')[0]\n",
        "      print(\"GPU not found, using CPU\")\n",
        "except ImportError:\n",
        "    # Handle cases outside Colab or where TPU tools aren't installed\n",
        "    if jax.devices('gpu'):\n",
        "      device = jax.devices('gpu')[0]\n",
        "      print(\"Using GPU\")\n",
        "    else:\n",
        "      # Fallback to CPU if no GPU/TPU detected\n",
        "      device = jax.devices('cpu')[0]\n",
        "      print(\"Using CPU\")\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "P41PKO1CoXRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 2. Data Preparation (In-Memory Focus)\n",
        "# --- Download and Read Data (Happens ONCE) ---\n",
        "# Download sample text data (\"The Verdict\" by Edith Wharton)\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/\"\n",
        "       \"main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
        "file_path = \"the-verdict.txt\"\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Downloading {file_path}...\")\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        text_data = response.read().decode('utf-8')\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(text_data)\n",
        "    print(\"Download complete.\")\n",
        "else:\n",
        "    print(f\"File {file_path} already exists.\")\n",
        "\n",
        "# Read the entire file content into RAM\n",
        "print(\"Reading data from disk into RAM...\")\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    text_data = file.read()\n",
        "print(f\"Loaded text data into RAM ({len(text_data)} characters)\")\n",
        "print(\"First 100 chars:\", text_data[:100])"
      ],
      "metadata": {
        "id": "nQkcVyslCsAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Tokenization using tiktoken ---\n",
        "# Initialize the GPT-2 tokenizer\n",
        "print(\"Initializing tokenizer...\")\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = tokenizer.n_vocab # Extract vocabulary size from the tokenizer\n",
        "print(f\"Tokenizer vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Convert the raw text data into a sequence of token IDs\n",
        "# This operation occurs in RAM.\n",
        "print(\"Encoding text data into token IDs (in RAM)...\")\n",
        "encoded_text = tokenizer.encode(text_data)\n",
        "\n",
        "# Convert the Python list of token IDs into a JAX array for efficient processing\n",
        "# This array resides in RAM or accelerator (GPU/TPU) memory.\n",
        "print(\"Converting token IDs to JAX array (in RAM/Device Memory)...\")\n",
        "encoded_text_jax = jnp.array(encoded_text, dtype=jnp.int32)\n",
        "print(f\"Encoded text stored as JAX array with shape: {encoded_text_jax.shape}\")\n",
        "print(encoded_text_jax[:50])"
      ],
      "metadata": {
        "id": "8f-jHaMxDD3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Create Training and Validation Sets ---\n",
        "# Split the encoded data into training (90%) and validation (10%) sets.\n",
        "# These are created as views (slices) of the main JAX array in memory, avoiding data duplication.\n",
        "print(\"Splitting data into train/validation sets (in RAM/Device Memory)...\")\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(encoded_text_jax))\n",
        "train_data = encoded_text_jax[:split_idx] # Slice for training data\n",
        "val_data = encoded_text_jax[split_idx:]   # Slice for validation data\n",
        "\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Validation data shape: {val_data.shape}\")\n",
        "\n",
        "# --- Data Loader / Batching Function ---\n",
        "# Define a function to generate batches for training or evaluation.\n",
        "# context_length: The number of tokens in each input sequence.\n",
        "# batch_size: The number of sequences processed in parallel per batch.\n",
        "def create_batches(data, batch_size, context_length, key):\n",
        "    \"\"\"\n",
        "    Generates batches of (input sequence, target sequence) pairs from the data.\n",
        "\n",
        "    Args:\n",
        "        data: A JAX array containing the token IDs (e.g., train_data or val_data).\n",
        "        batch_size: Number of sequences per batch.\n",
        "        context_length: Length of each input sequence.\n",
        "        key: JAX PRNG key for shuffling.\n",
        "\n",
        "    Yields:\n",
        "        tuple: A batch containing (x_batch, y_batch).\n",
        "               x_batch: Input sequences (batch_size, context_length).\n",
        "               y_batch: Target sequences (batch_size, context_length), shifted by one token.\n",
        "\n",
        "    Note: Operates entirely on the provided JAX array in memory, using slicing and stacking.\n",
        "          It does not perform disk I/O. Implicitly shuffles data via random starting indices.\n",
        "    \"\"\"\n",
        "    # Total number of possible starting positions for sequences\n",
        "    num_sequences = len(data) - context_length\n",
        "    if num_sequences <= 0:\n",
        "        raise ValueError(\"Dataset is too small for the given context length.\")\n",
        "\n",
        "    # Generate a random permutation of starting indices for sequences\n",
        "    idxs = jax.random.permutation(key, num_sequences)\n",
        "\n",
        "    # Calculate the number of full batches that can be created\n",
        "    # Drops the last potentially smaller batch to ensure consistent batch shapes,\n",
        "    # which is important for efficient JIT compilation.\n",
        "    num_batches = num_sequences // batch_size\n",
        "\n",
        "    print(f\"Creating batches: {num_batches} batches of size {batch_size}...\")\n",
        "    # Iterate through the indices to form batches\n",
        "    for i in range(num_batches):\n",
        "        # Select indices for the current batch\n",
        "        batch_idxs = idxs[i * batch_size : (i + 1) * batch_size]\n",
        "        # Create input sequences by slicing the data array\n",
        "        x_batch = jnp.stack([data[idx : idx + context_length] for idx in batch_idxs])\n",
        "        # Create target sequences (input shifted by one token) by slicing\n",
        "        y_batch = jnp.stack([data[idx + 1 : idx + context_length + 1] for idx in batch_idxs])\n",
        "        yield x_batch, y_batch"
      ],
      "metadata": {
        "id": "bb_8Dsb3DOGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Configuration ---\n",
        "# Define hyperparameters for the transformer model and training\n",
        "config = {\n",
        "    \"vocab_size\": vocab_size,       # Size of the token vocabulary\n",
        "    \"context_length\": 128,      # Max sequence length for the model\n",
        "    \"emb_dim\": 64,             # Dimension of token embeddings\n",
        "    \"n_heads\": 4,               # Number of attention heads\n",
        "    \"n_layers\": 4,              # Number of transformer blocks (layers)\n",
        "    \"drop_rate\": 0.1,           # Dropout rate for regularization\n",
        "    \"qkv_bias\": False,          # Whether to use bias in QKV projections\n",
        "    \"batch_size\": 32,          # Number of sequences per training batch\n",
        "}\n",
        "\n",
        "# --- Demonstrate Batch Generation ---\n",
        "# Create a JAX PRNG key for reproducibility in batch generation\n",
        "data_key = random.PRNGKey(0)\n",
        "# Create a generator for training batches using the function and training data\n",
        "# The train_data array is already in memory.\n",
        "batch_generator = create_batches(train_data, config[\"batch_size\"], config[\"context_length\"], data_key)\n",
        "# Get the first batch from the generator\n",
        "x_example, y_example = next(batch_generator)\n",
        "\n",
        "# Print shapes and examples from the first batch\n",
        "print(\"\\nExample Input Batch Shape:\", x_example.shape)\n",
        "print(\"Example Target Batch Shape:\", y_example.shape)\n",
        "print(\"Example Input Batch (first 5 tokens):\", x_example[0, :5])\n",
        "print(\"Example Target Batch (first 5 tokens):\", y_example[0, :5])"
      ],
      "metadata": {
        "id": "kjtm7c3DqpmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 3. Transformer Components (JAX/Flax)\n",
        "\n",
        "class TokenAndPositionalEmbedding(nn.Module):\n",
        "    \"\"\"Combines token embeddings and learnable absolute positional embeddings.\"\"\"\n",
        "    vocab_size: int      # Number of unique tokens in the vocabulary\n",
        "    embed_dim: int       # Dimension of the embedding vectors\n",
        "    context_length: int  # Maximum sequence length the model handles\n",
        "\n",
        "    def setup(self):\n",
        "        self.tok_emb = nn.Embed(num_embeddings=self.vocab_size, features=self.embed_dim)\n",
        "        # Learnable absolute positional embeddings\n",
        "        self.pos_emb = nn.Embed(num_embeddings=self.context_length, features=self.embed_dim)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for combining token and positional embeddings.\n",
        "\n",
        "        Args:\n",
        "            x: Input token IDs, shape (batch_size, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            Combined embeddings (token + position), shape (batch_size, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        seq_len = x.shape[1]\n",
        "        #### Compelete Code here #######\n",
        "        # ~ 3 lines\n",
        "\n",
        "        ###################################\n",
        "        # Add token and positional embeddings\n",
        "        combined_embeddings = token_embeddings + position_embeddings\n",
        "        return combined_embeddings\n",
        "\n",
        "# --- Multi-Head Causal Self-Attention ---\n",
        "class MultiHeadCausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head self-attention with causal masking, using separate Q, K, V projections.\n",
        "    \"\"\"\n",
        "    embed_dim: int       # Total dimension of the embedding (e.g., 512)\n",
        "    num_heads: int       # Number of attention heads (e.g., 8)\n",
        "    use_bias: bool = False  # Whether to use bias in projections\n",
        "    dropout_rate: float = 0.1 # Dropout rate\n",
        "\n",
        "    def setup(self):\n",
        "        assert self.embed_dim % self.num_heads == 0, \"Embed dim must be divisible by num_heads\"\n",
        "        self.head_dim = self.embed_dim // self.num_heads # Dim per head (e.g., 64)\n",
        "\n",
        "        # --- Separate Linear Projections for Q, K, V ---\n",
        "        # Each projects from embed_dim to embed_dim\n",
        "\n",
        "        self.q_proj = nn.Dense(features=self.embed_dim, use_bias=self.use_bias, name=\"query_proj\")\n",
        "        self.k_proj = nn.Dense(features=self.embed_dim, use_bias=self.use_bias, name=\"key_proj\")\n",
        "        self.v_proj = nn.Dense(features=self.embed_dim, use_bias=self.use_bias, name=\"value_proj\")\n",
        "\n",
        "        # Output projection after combining heads\n",
        "        self.out_proj = nn.Dense(features=self.embed_dim, use_bias=self.use_bias, name=\"output_proj\")\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout_attn = nn.Dropout(rate=self.dropout_rate) # Dropout on attention weights\n",
        "        self.dropout_out = nn.Dropout(rate=self.dropout_rate)  # Dropout on final output\n",
        "\n",
        "    def __call__(self, x, deterministic: bool):\n",
        "        \"\"\"\n",
        "        Forward pass for multi-head causal self-attention.\n",
        "\n",
        "        Args:\n",
        "            x: Input embeddings, shape (batch_size, seq_len, embed_dim)\n",
        "            deterministic: If True, disables dropout (used during inference).\n",
        "\n",
        "        Returns:\n",
        "            Output context vectors after attention, shape (batch_size, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # --- Step 1: Project Input to Q, K, V Separately ---\n",
        "\n",
        "        #### Compelete code here ########\n",
        "        # ~ 3lines\n",
        "\n",
        "\n",
        "        ##################################\n",
        "\n",
        "        # --- Step 2: Split into Heads ---\n",
        "        # Reshape and transpose each of Q, K, V for multi-head processing\n",
        "        # 'b s (h d)' means: batch, sequence, (num_heads * head_dim)\n",
        "        # '-> b h s d' means: transform into (batch, num_heads, sequence, head_dim)\n",
        "        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)\n",
        "        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads)\n",
        "        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads)\n",
        "        # Now q, k, v have shape: (batch, num_heads, seq_len, head_dim)\n",
        "\n",
        "        # --- Step 3: Calculate Attention Scores ---\n",
        "        # Scaled Dot-Product Attention: Q @ K^T / sqrt(head_dim)\n",
        "        # MatMul: (b, h, s, d) @ (b, h, d, s) -> (b, h, s, s)\n",
        "\n",
        "\n",
        "        #### Complete Code Here ############\n",
        "        # ~ 2-3 line\n",
        "        # compute attention scores, and normalize them\n",
        "\n",
        "        ####################################\n",
        "\n",
        "        # --- Step 4: Apply Causal Mask ---\n",
        "        # Prevent attending to future tokens\n",
        "        mask = nn.make_causal_mask(jnp.ones((batch_size, seq_len)), dtype=jnp.bool_)\n",
        "        # Set masked positions to -inf before softmax\n",
        "        # Mask shape is broadcastable: (batch_size, 1, seq_len, seq_len)\n",
        "        attn_scores = jnp.where(mask, attn_scores, -jnp.inf)\n",
        "\n",
        "        # --- Step 5: Calculate Attention Weights ---\n",
        "        # Softmax converts scores to probabilities along the key sequence length dimension\n",
        "        attn_weights = jax.nn.softmax(attn_scores, axis=-1) # Shape: (b, h, s, s)\n",
        "        attn_weights = self.dropout_attn(attn_weights, deterministic=deterministic)\n",
        "\n",
        "        # --- Step 6: Calculate Context Vectors ---\n",
        "        # Weighted sum of Value vectors: Weights @ V\n",
        "        # MatMul: (b, h, s, s) @ (b, h, s, d) -> (b, h, s, d)\n",
        "        context_vec = jnp.matmul(attn_weights, v) # Shape: (batch, num_heads, seq_len, head_dim)\n",
        "\n",
        "        # --- Step 7: Combine Heads ---\n",
        "        # Rearrange back to merge head dimension into embedding dimension\n",
        "        # 'b h s d' -> 'b s (h d)' which is (batch, seq_len, embed_dim)\n",
        "        context_combined = rearrange(context_vec, 'b h s d -> b s (h d)')\n",
        "\n",
        "        # --- Step 8: Final Output Projection ---\n",
        "        # Apply final linear layer to mix head information\n",
        "        output = self.out_proj(context_combined) # Shape: (batch, seq_len, embed_dim)\n",
        "        output = self.dropout_out(output, deterministic=deterministic)\n",
        "\n",
        "        return output\n",
        "# --- Feed Forward Network (Position-wise) ---\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"A simple two-layer feed-forward network applied position-wise.\"\"\"\n",
        "    embed_dim: int       # Input and output dimension\n",
        "    dropout_rate: float = 0.1 # Dropout rate\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, deterministic: bool):\n",
        "        \"\"\"\n",
        "        Forward pass for the feed-forward network.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor, shape (batch_size, seq_len, embed_dim)\n",
        "            deterministic: If True, disables dropout.\n",
        "\n",
        "        Returns:\n",
        "            Output tensor, shape (batch_size, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        # Typically expand to 4x embed_dim in the hidden layer\n",
        "        hidden_dim = 4 * self.embed_dim\n",
        "        ############## Complete code here ##############\n",
        "        ## apply FFW block\n",
        "        # ~ 3lines\n",
        "\n",
        "        ################################################\n",
        "        x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)\n",
        "        return x\n",
        "\n",
        "# --- Transformer Block ---\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"A single block of the Transformer architecture (Pre-LN variant).\"\"\"\n",
        "    embed_dim: int       # Embedding dimension\n",
        "    num_heads: int       # Number of attention heads\n",
        "    use_bias: bool       # Whether to use bias in projections\n",
        "    dropout_rate: float  # Dropout rate\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, deterministic: bool):\n",
        "        \"\"\"\n",
        "        Forward pass for a Transformer block using Pre-Layer Normalization.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor, shape (batch_size, seq_len, embed_dim)\n",
        "            deterministic: If True, disables dropout.\n",
        "\n",
        "        Returns:\n",
        "            Output tensor, shape (batch_size, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        # --- Attention Sub-layer (Pre-LN) ---\n",
        "        # y = gamma * (x - mean(x)) / sqrt(variance(x) + epsilon) + beta\n",
        "\n",
        "        ###################### Complete the block  ######################\n",
        "        ## Attention part\n",
        "\n",
        "\n",
        "        x = x + attn_output # Residual connection\n",
        "        #################################################\n",
        "\n",
        "\n",
        "        # --- Feed Forward Sub-layer (Pre-LN) ---\n",
        "        ###################### Complete the block  ######################\n",
        "        ## Feed Forward part\n",
        "\n",
        "\n",
        "        x = x + ffn_output # Residual connection\n",
        "        #################################################\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "TVDw1I02rTo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 4. GPT Model Architecture (JAX/Flax)\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"Defines the GPT (Generative Pre-trained Transformer) model architecture.\"\"\"\n",
        "    vocab_size: int      # Size of the vocabulary\n",
        "    embed_dim: int       # Dimension of token and positional embeddings\n",
        "    context_length: int  # Maximum sequence length the model can process\n",
        "    num_heads: int       # Number of attention heads in each Transformer block\n",
        "    num_layers: int      # Number of Transformer blocks stacked\n",
        "    use_bias: bool       # Whether to use bias in linear layers (Dense, projections)\n",
        "    dropout_rate: float  # Dropout rate for regularization\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, idx, deterministic: bool):\n",
        "        \"\"\"\n",
        "        Forward pass of the GPT model.\n",
        "\n",
        "        Args:\n",
        "            idx: Input token indices, shape (batch_size, seq_len).\n",
        "            deterministic: If True, disables dropout (used during inference).\n",
        "\n",
        "        Returns:\n",
        "            logits: Output logits over the vocabulary, shape (batch_size, seq_len, vocab_size).\n",
        "        \"\"\"\n",
        "        # 1. Input Embeddings (Combine Token and Positional Embeddings)\n",
        "        # Input: (batch, seq_len) -> Output: (batch, seq_len, embed_dim)\n",
        "        x = TokenAndPositionalEmbedding(\n",
        "            vocab_size=self.vocab_size,\n",
        "            embed_dim=self.embed_dim,\n",
        "            context_length=self.context_length,\n",
        "            name=\"embedding\" # Added name for clarity\n",
        "        )(idx)\n",
        "        # Apply dropout to the combined embeddings (if training)\n",
        "        x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)\n",
        "\n",
        "        # 2. Stacked Transformer Blocks\n",
        "        # Process the sequence through multiple layers of Transformer blocks.\n",
        "        # Input/Output shape for each block: (batch, seq_len, embed_dim)\n",
        "        for i in range(self.num_layers):\n",
        "            x = TransformerBlock(\n",
        "                embed_dim=self.embed_dim,\n",
        "                num_heads=self.num_heads,\n",
        "                use_bias=self.use_bias,\n",
        "                dropout_rate=self.dropout_rate,\n",
        "                name=f\"transformer_block_{i}\" # Added name for clarity\n",
        "            )(x, deterministic=deterministic)\n",
        "\n",
        "        # 3. Final Layer Normalization (applied after the last block)\n",
        "        # Stabilizes the inputs to the final linear layer.\n",
        "        x = nn.LayerNorm(epsilon=1e-5, name=\"final_ln\")(x) # Added name for clarity\n",
        "\n",
        "        # 4. Output Head (Linear layer to map to vocabulary size)\n",
        "        # Projects the final transformer output to vocabulary-sized logits.\n",
        "        # Input: (batch, seq_len, embed_dim) -> Output: (batch, seq_len, vocab_size)\n",
        "        # Note: Weight tying between embedding and output layer is not used here.\n",
        "        logits = nn.Dense(features=self.vocab_size, use_bias=False, name=\"output_projection\")(x)\n",
        "        # logits represent the unnormalized scores for each token in the vocabulary\n",
        "        # at each position in the sequence.\n",
        "        return logits"
      ],
      "metadata": {
        "id": "K6s8_CZ5rWPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## 5. Pre-training Loop (Next Token Prediction)\n",
        "\n",
        "# --- Loss Function (Cross-Entropy) ---\n",
        "@functools.partial(jax.jit) # JIT compile for speed\n",
        "def cross_entropy_loss(logits, targets):\n",
        "    \"\"\"Calculates average cross-entropy loss for language modeling.\"\"\"\n",
        "    # logits: (batch, seq_len, vocab_size)\n",
        "    # targets: (batch, seq_len)\n",
        "\n",
        "    # Convert targets to one-hot encoding\n",
        "    one_hot_targets = jax.nn.one_hot(targets, num_classes=logits.shape[-1])\n",
        "    # Calculate log-probabilities (log-softmax is numerically stable)\n",
        "    log_softmax_logits = jax.nn.log_softmax(logits, axis=-1)\n",
        "    # Calculate loss per position\n",
        "    loss_per_position = -jnp.sum(one_hot_targets * log_softmax_logits, axis=-1)\n",
        "    # Average loss over batch and sequence length\n",
        "    return jnp.mean(loss_per_position)"
      ],
      "metadata": {
        "id": "7XfMT3eReTWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Step ---\n",
        "# JIT compile the training step for performance.\n",
        "# `model_apply` and `learning_rate_fn` are static to prevent recompilation.\n",
        "@functools.partial(jax.jit, static_argnames=['model_apply', 'learning_rate_fn'])\n",
        "def train_step(state, batch, dropout_key, model_apply, learning_rate_fn):\n",
        "    \"\"\"Performs a single gradient update step.\"\"\"\n",
        "    x, y = batch\n",
        "\n",
        "    # Define loss function for gradient calculation\n",
        "    def compute_loss(params):\n",
        "        # Forward pass with dropout enabled\n",
        "        logits = model_apply({'params': params}, x, deterministic=False, rngs={'dropout': dropout_key})\n",
        "        loss = cross_entropy_loss(logits, y)\n",
        "        return loss\n",
        "\n",
        "    # Compute loss and gradients\n",
        "    grad_fn = jax.value_and_grad(compute_loss)\n",
        "    loss, grads = grad_fn(state.params)\n",
        "\n",
        "    # Update model state (apply gradients, update optimizer state, increment step)\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "\n",
        "    # Collect metrics\n",
        "    lr = learning_rate_fn(state.step)\n",
        "    metrics = {'loss': loss, 'learning_rate': lr}\n",
        "    return state, metrics\n",
        "\n",
        "# --- Evaluation Step ---\n",
        "# JIT compile the evaluation step. `model_apply` is static.\n",
        "@functools.partial(jax.jit, static_argnames=['model_apply'])\n",
        "def eval_step(state, batch, model_apply):\n",
        "    \"\"\"Performs a single evaluation step (no gradients).\"\"\"\n",
        "    x, y = batch\n",
        "    # Forward pass with dropout disabled\n",
        "    logits = model_apply({'params': state.params}, x, deterministic=True)\n",
        "    loss = cross_entropy_loss(logits, y)\n",
        "    # Return evaluation loss\n",
        "    return {'loss': loss}"
      ],
      "metadata": {
        "id": "Vg5yVFf0rZKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Optimizer and Train State ---\n",
        "\n",
        "# Configure the AdamW optimizer\n",
        "learning_rate = 1e-4\n",
        "tx = optax.adamw(learning_rate=learning_rate, weight_decay=0.1)\n",
        "\n",
        "# Generate JAX PRNG keys for reproducible initialization\n",
        "model_key, params_key, dropout_key_init = random.split(random.PRNGKey(123), 3)\n",
        "\n",
        "# Instantiate the GPT model\n",
        "model = GPT(\n",
        "    vocab_size=config[\"vocab_size\"],\n",
        "    embed_dim=config[\"emb_dim\"],\n",
        "    context_length=config[\"context_length\"],\n",
        "    num_heads=config[\"n_heads\"],\n",
        "    num_layers=config[\"n_layers\"],\n",
        "    use_bias=config[\"qkv_bias\"],\n",
        "    dropout_rate=config[\"drop_rate\"]\n",
        ")\n",
        "\n",
        "# Initialize model parameters using a dummy input and the params_key.\n",
        "# `deterministic=True` disables dropout during initialization.\n",
        "dummy_input = jnp.ones((1, config[\"context_length\"]), dtype=jnp.int32)\n",
        "params = model.init(params_key, dummy_input, deterministic=True)['params']\n",
        "\n",
        "# Create the training state to bundle model apply function, parameters, and optimizer state.\n",
        "state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
        "\n",
        "# Move the training state to the specified JAX device (GPU/TPU/CPU).\n",
        "state = jax.device_put(state, device)\n",
        "\n",
        "# Print the total number of model parameters.\n",
        "param_count = sum(p.size for p in jax.tree_util.tree_leaves(state.params))\n",
        "print(f\"Model initialized with {param_count:,} parameters.\")"
      ],
      "metadata": {
        "id": "zyXIEVYYrb8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Training Loop ---\n",
        "num_epochs = 1 # Number of full passes over the training data\n",
        "eval_frequency = 1000 # How often to evaluate on validation data (in steps)\n",
        "\n",
        "# Main PRNG key for the training loop\n",
        "train_key = random.PRNGKey(42)\n",
        "\n",
        "print(f\"Starting training for {num_epochs} epochs...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"--- Epoch {epoch+1}/{num_epochs} ---\")\n",
        "    epoch_train_loss = 0.0\n",
        "    num_train_batches = 0\n",
        "\n",
        "    # Create a new batch generator for each epoch with a unique key for shuffling\n",
        "    epoch_key, train_key = random.split(train_key)\n",
        "    batch_generator = create_batches(train_data, config[\"batch_size\"], config[\"context_length\"], epoch_key)\n",
        "\n",
        "    # Calculate total steps for the tqdm progress bar\n",
        "    # Note: This calculation should match the logic inside create_batches\n",
        "    num_sequences = len(train_data) - config[\"context_length\"]\n",
        "    total_steps_per_epoch = num_sequences // config[\"batch_size\"] # Drops partial batch, matching create_batches\n",
        "\n",
        "    # Wrap the batch generator with tqdm for a progress bar\n",
        "    pbar = tqdm(enumerate(batch_generator),\n",
        "                total=total_steps_per_epoch,\n",
        "                desc=f\"Epoch {epoch+1} Training\")\n",
        "\n",
        "    # Iterate over training batches for the current epoch\n",
        "    for step, train_batch in pbar:\n",
        "        # Move the current training batch to the target device\n",
        "        train_batch = jax.device_put(train_batch, device)\n",
        "\n",
        "        # Generate a unique dropout key for this specific training step\n",
        "        dropout_key_step = random.fold_in(dropout_key_init, state.step)\n",
        "\n",
        "        # Perform a single training step\n",
        "        state, train_metrics = train_step(state, train_batch, dropout_key_step, model.apply, lambda step: learning_rate)\n",
        "        epoch_train_loss += train_metrics['loss']\n",
        "        num_train_batches += 1\n",
        "\n",
        "        # --- Logging and Evaluation Periodically ---\n",
        "        if (step + 1) % eval_frequency == 0:\n",
        "            # Calculate average training loss over the interval\n",
        "            # Use jax.device_get to bring scalar loss values back from GPU/TPU if needed for calculation/display\n",
        "            avg_train_loss = jax.device_get(epoch_train_loss / num_train_batches)\n",
        "\n",
        "            # Evaluate performance on the validation set\n",
        "            val_loss = 0.0\n",
        "            num_val_batches = 0\n",
        "            val_key, train_key = random.split(train_key)\n",
        "            val_batch_generator = create_batches(val_data, config[\"batch_size\"], config[\"context_length\"], val_key)\n",
        "            # Optional: Wrap validation loop in tqdm as well\n",
        "            # val_pbar = tqdm(val_batch_generator, desc=\"Validation\", leave=False)\n",
        "            # for val_batch in val_pbar:\n",
        "            for val_batch in val_batch_generator:\n",
        "                val_batch = jax.device_put(val_batch, device)\n",
        "                eval_metrics = eval_step(state, val_batch, model.apply)\n",
        "                val_loss += eval_metrics['loss']\n",
        "                num_val_batches += 1\n",
        "\n",
        "            # Calculate average validation loss\n",
        "            avg_val_loss = jax.device_get(val_loss / num_val_batches) if num_val_batches > 0 else 0.0\n",
        "\n",
        "            # Update the progress bar postfix with the latest loss values\n",
        "            pbar.set_postfix(TrainLoss=f\"{avg_train_loss:.4f}\", ValLoss=f\"{avg_val_loss:.4f}\")\n",
        "\n",
        "            # Log performance metrics (using print might interfere slightly with tqdm, but often ok)\n",
        "            # Alternatively, use tqdm.write()\n",
        "            # tqdm.write(f\"  Step: {state.step:>5} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {train_metrics['learning_rate']:.6f}\")\n",
        "            print(f\"\\n  Step: {state.step:>5} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {jax.device_get(train_metrics['learning_rate']):.6f}\")\n",
        "\n",
        "            # Reset training loss accumulator for the next evaluation interval\n",
        "            epoch_train_loss = 0.0\n",
        "            num_train_batches = 0\n",
        "\n",
        "    # Ensure the progress bar finishes cleanly for the epoch\n",
        "    pbar.close()\n",
        "    print(f\"--- Epoch {epoch+1} finished ---\")\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "id": "0rWHxpumre-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KZ0szqqnvzj"
      },
      "outputs": [],
      "source": [
        "#@title ## 6. Text Generation\n",
        "\n",
        "# JIT compile for speed, static args prevent recompilation for config changes.\n",
        "@functools.partial(jax.jit, static_argnames=['model_apply', 'max_new_tokens', 'context_length', 'temperature', 'top_k'])\n",
        "def generate_text(state, prompt_ids, max_new_tokens, context_length, model_apply, temperature=1.0, top_k=None, key=random.PRNGKey(0)):\n",
        "    \"\"\"Generates text autoregressively from a prompt.\"\"\"\n",
        "    current_ids = prompt_ids\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Truncate context to the model's maximum length\n",
        "        context_ids = current_ids[:, -context_length:]\n",
        "\n",
        "        # Get logits using the model in evaluation mode (deterministic=True)\n",
        "        logits = model_apply({'params': state.params}, context_ids, deterministic=True)\n",
        "\n",
        "        # Use only the logits for the last token to predict the next token\n",
        "        last_token_logits = logits[:, -1, :] # Shape: (1, vocab_size)\n",
        "\n",
        "        # --- Sampling ---\n",
        "        if temperature <= 0:\n",
        "            # Greedy sampling: take the most probable token\n",
        "            next_token_id = jnp.argmax(last_token_logits, axis=-1)\n",
        "        else:\n",
        "            # Temperature scaling adjusts randomness\n",
        "            scaled_logits = last_token_logits / temperature\n",
        "\n",
        "            # Optional Top-K sampling limits choices to the K most likely tokens\n",
        "            if top_k is not None:\n",
        "                top_logits, _ = jax.lax.top_k(scaled_logits, k=top_k)\n",
        "                min_top_logit = top_logits[:, -1]\n",
        "                # Mask logits below the top K threshold\n",
        "                mask = scaled_logits < min_top_logit\n",
        "                scaled_logits = jnp.where(mask, -jnp.inf, scaled_logits)\n",
        "\n",
        "            # Sample from the adjusted probability distribution\n",
        "            key, subkey = random.split(key)\n",
        "            next_token_id = random.categorical(subkey, scaled_logits, axis=-1) # Shape: (1,)\n",
        "\n",
        "        # Append the sampled token ID to the sequence\n",
        "        current_ids = jnp.concatenate([current_ids, next_token_id[:, None]], axis=1)\n",
        "\n",
        "    return current_ids"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Generate Example Text ---\n",
        "start_context = \"Hello, I am\"\n",
        "start_ids = jnp.array(tokenizer.encode(start_context), dtype=jnp.int32)[None, :] # Add batch dim\n",
        "start_ids = jax.device_put(start_ids, device) # Move prompt to device\n",
        "\n",
        "print(f\"\\nGenerating text starting with: '{start_context}'\")\n",
        "\n",
        "# Use the trained state\n",
        "generation_key = random.PRNGKey(567)\n",
        "generated_ids = generate_text(\n",
        "    state=state,\n",
        "    prompt_ids=start_ids,\n",
        "    max_new_tokens=10,\n",
        "    context_length=config[\"context_length\"],\n",
        "    model_apply=model.apply,\n",
        "    temperature=0.7, # Add some randomness\n",
        "    top_k=50,        # Consider top 50 tokens\n",
        "    key=generation_key\n",
        ")\n",
        "\n",
        "# Decode the generated token IDs back to text\n",
        "generated_text = tokenizer.decode(generated_ids[0].tolist()) # Remove batch dim before decoding\n",
        "print(\"\\nGenerated Text:\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "3KTS9IyKySmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Conclusion\n",
        "\n",
        "This notebook demonstrated the fundamentals of building and training a decoder-only Transformer for language modeling using JAX and Flax:\n",
        "\n",
        "1.  Text data preparation and tokenization.\n",
        "2.  Implementation of core Transformer components (Embeddings, Attention, LayerNorm, FFN).\n",
        "3.  Construction of the full GPT model architecture.\n",
        "4.  Implementation of a next-token prediction pre-training loop.\n",
        "5.  Basic training on a single accelerator.\n",
        "6.  Autoregressive text generation with the trained model.\n",
        "\n",
        "This provides a foundation for understanding how such language models work.\n",
        "\n",
        "Further steps to explore could include:\n",
        "* Training for more epochs or using larger datasets.\n",
        "* Implementing more sophisticated data loading pipelines.\n",
        "* Experimenting with different hyperparameters (model size, learning rate, etc.).\n",
        "* Adding techniques like learning rate scheduling or gradient clipping."
      ],
      "metadata": {
        "id": "MYM8uqb9gyBT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hFL4JH2Dg1YO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}